\documentclass{beamer}
\usetheme{metropolis}           % Use metropolis theme
\usepackage{appendixnumberbeamer}
\usepackage{epigraph}
\usepackage{color}
\usepackage{amsopn}
\usepackage{tabto}
\usepackage{pbox}           % table line break
\usepackage{algorithm}      % algorithm
\usepackage[noend]{algpseudocode} % algorithm
\usepackage{bm}             % bold in math
\usepackage{mathtools}      % dcases*

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{makecell}

%%% Bibliography
\usepackage[backend=bibtex, style=authoryear]{biblatex}
\AtBeginBibliography{\tiny}
\bibliography{../bibliography.bib}

\setcounter{tocdepth}{1}      % hide subsections in Table of Contents
\renewcommand{\thealgorithm}{} % disable numbering of algorithms

\setbeamercolor{background canvas}{bg=white}
\setbeamercolor{title}{fg=black}
\setbeamercolor{subtitle}{fg=black}
\setbeamercolor{date}{fg=black}
\setbeamercolor{author}{fg=black}
\setbeamercolor{institute}{fg=black}

\newcommand{\todo}{\alert{TODO}}
\newcommand{\itemBullet}{\scriptsize$\blacksquare$}
\setbeamertemplate{itemize item}{\itemBullet}
\setbeamertemplate{itemize subitem}{\itemBullet}
\setbeamertemplate{itemize subsubitem}{\itemBullet}
\newcommand{\E}{\mathop{\mathbb{E}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\epiParSpace}{\vskip 1.5ex}
\newcommand{\p}{\mathbf{p}}
\newcommand{\colorcite}{\color{gray!35}}
\newcolumntype{?}[1]{!{\vrule width #1}}
\newcommand{\hbline}{\Xhline{2.5\arrayrulewidth}}
\newcommand\nocell[1]{\multicolumn{#1}{c?{1.5pt}}{}}
\setcellgapes{5pt}

\newcommand{\atariCitation}{\colorcite[\cite{Mnih2015human}]}
\newcommand{\cepheusCitation}{\colorcite[\cite{Bowling2015heads}]}
\newcommand{\alphaGoCitation}{\colorcite[\cite{Silver2016mastering}]}
\newcommand{\searchTreeCitation}{\colorcite[\cite{Allis1994searching}]}
\newcommand{\nnModesCitation}{\colorcite[\cite{Dieterle2003multianalyte}]}

\title{\Huge AI Supremacy in Games}
\subtitle{\small Deep Blue, Watson, Cepheus, AlphaGo, DeepStack and TensorCFR}
\date{13\textsuperscript{th} June 2018}
\author{Karel Ha}
\institute{Artificial Intelligence Center\\
  Czech Technical University in Prague}

\begin{document}
  {
    \usebackgroundtemplate{
      \includegraphics[height=\paperheight]{../img/hands-touch-of-god.jpg}
    }
    \maketitle
  }

  \begin{frame}{The Outline}
    \tableofcontents
  \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{General AI: One Summer Dream}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{AI in Games}
  \begin{frame}[standout]
    \includegraphics[height=\paperheight]{../img/game_AIs.png}
    \nocite{xkcdGameAIs}
  \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{Chess: Deep Blue}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{Jeopardy!: Watson}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{Atari Games:\\
    Deep Reinforcement Learning}
  {
    \setbeamertemplate{frame footer}{\atariCitation}
    \begin{frame}{Atari Player by Google DeepMind}
      \begin{center}
        \includegraphics[width=\textwidth, height=\textheight, keepaspectratio]{../img/atari_breakout.jpg}

        \url{https://youtu.be/0X-NdPtFKq0?t=21m13s}
      \end{center}
    \end{frame}
  }

  {
    \setbeamertemplate{frame footer}{\url{https://youtu.be/0X-NdPtFKq0?t=16m57s}}
    \begin{frame}{Reinforcement Learning}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/RL_framework.png}
      \end{center}
      \pause
      games of \textbf{self-play}
    \end{frame}
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{Go:\\
    AlphaGo, AlphaGo Zero, AlphaZero}

  \begin{frame}[standout]
    \begin{center}
      Crash Course:\\
      Tree Search
    \end{center}
  \end{frame}

  {
    \setbeamertemplate{frame footer}{\alphaGoCitation}
    \begin{frame}{Tree Search}
      Optimal value~$v^*(s)$ determines the~outcome of~the game:
      \pause
      \begin{itemize}[<+- | alert@+>]
          \tiny
        \item from every board position or state $s$
        \item under perfect play by~all players.
      \end{itemize}
      \pause

      It is computed by~\textbf{recursively traversing a~search tree} containing approximately $b^d$ possible sequences of moves, where
      \pause
      \begin{itemize}[<+- | alert@+>]
          \tiny
        \item $b$ is the game’s breadth (number of legal moves per position)
        \item $d$ is its depth (game length)
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{frame footer}{\searchTreeCitation}
    \begin{frame}{Game tree of~Go}
      Sizes of~trees for~various games:
      \begin{itemize}
        \item chess: $b \approx 35, d \approx 80$
        \item Go: $b \approx 250, d \approx 150$
          \pause
          $\Rightarrow$ more positions than atoms in the universe!
      \end{itemize}
      \pause

      \epigraph{
        That makes Go a \textbf{googol} times more complex than chess.
      }{\tiny\url{https://deepmind.com/alpha-go.html}}
      \pause

      \vskip -1em
      How to handle the size~of the game tree?
      \pause
      \begin{itemize}[<+- | alert@+>]
        \item for the breadth: a~neural network to~select moves
        \item for the depth: a~neural network to~evaluate current position
        \item for the tree traverse: Monte Carlo tree search (MCTS)
      \end{itemize}
    \end{frame}
  }

  \begin{frame}{Monte Carlo tree search}
    \includegraphics[width=\textwidth]{../img/MCTS.png}
  \end{frame}

  \begin{frame}[standout]
    \begin{center}
      Crash Course:\\
      Neural Networks
    \end{center}
  \end{frame}

  {
    \setbeamertemplate{frame footer}{\url{http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html}}
    \begin{frame}{Neural Network: Inspiration}
      \vskip -2em
      \begin{center}
        \includegraphics[height=.5\textheight]{../img/colored_neural_network.png}
      \end{center}

      \pause
      \begin{itemize}[<+- | alert@+>]
        \item inspired by~the neuronal structure of~the mammalian cerebral cortex
        \item but on~much smaller scales
        \item suitable to model systems with a~high tolerance to~error 
          \begin{itemize}
            \item e.g.~audio or~image recognition
          \end{itemize}
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{frame footer}{\nnModesCitation}
    \begin{frame}{Neural Network: Modes}
      \begin{center}
        \includegraphics[height=.6\textheight]{../img/neural_network_forward_and_backprop.png}
      \end{center}

      \pause
      Two modes
      \pause
      \begin{itemize}[<+- | alert@+>]
        \item \textbf{feedforward} for making predictions
        \item \textbf{backpropagation} for learning
      \end{itemize}
    \end{frame}
  }

  {
    \setbeamertemplate{frame footer}{\url{http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html}}

    \begin{frame}{(Deep) Convolutional Neural Network}
      \pause
      \begin{center}
        \includegraphics[width=\textwidth]{../img/ConvNet_hierarchy.png}
      \end{center}
    \end{frame}
  }

  \begin{frame}[standout]
    \begin{center}
      AlphaGo
    \end{center}
  \end{frame}

  {
    \setbeamertemplate{frame footer}{\alphaGoCitation}
    \begin{frame}{Policy and Value Networks}
      \begin{center}
        \includegraphics[height=.85\textheight]{../img/policy_and_value_network.png}
      \end{center}
    \end{frame}

    \begin{frame}{Training the (Deep Convolutional) Neural Networks}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/neural_nets_pipeline.png}
      \end{center}
    \end{frame}

    \begin{frame}{SL Policy Networks}
      \begin{center}
        \includegraphics[height=.8\textheight]{../img/eval_SL_policy_network.png}

        \tiny
        move probabilities taken directly from the SL policy network $p_\sigma$ (reported as a~percentage if above $0.1\%$).
      \end{center}
    \end{frame}

    \begin{frame}{Training the (Deep Convolutional) Neural Networks}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/neural_nets_pipeline.png}
      \end{center}
    \end{frame}

    \begin{frame}{Rollout Policy}
      \begin{itemize}[<+- | alert@+>]
        \item Rollout policy~$p_\pi(a|s)$ is \textbf{faster} but \textbf{less accurate} than SL policy network.
        \item It takes $2 \mu$s to~select an~action, compared to $3$~ms in~case of~SL policy network.
      \end{itemize}
    \end{frame}

    \begin{frame}{Training the (Deep Convolutional) Neural Networks}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/neural_nets_pipeline.png}
      \end{center}
    \end{frame}

    \begin{frame}{RL Policy Networks}
      \begin{itemize}[<+- | alert@+>]
        \item identical in~structure to the SL policy network
        \item games of \textbf{self-play}
          \begin{itemize}[<+- | alert@+>]
            \item between the current RL policy network and a~randomly selected previous iteration
            \item goal: to~win in~the games of~self-play
          \end{itemize}
      \end{itemize}
      \pause
    \end{frame}

    \begin{frame}{Training the (Deep Convolutional) Neural Networks}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/neural_nets_pipeline.png}
      \end{center}
    \end{frame}

    \begin{frame}{Value Network (1/2)}
      \begin{itemize}[<+- | alert@+>]
        \item similar architecture to the policy network, but outputs a~single prediction instead of~a~probability distribution
        \item goal: to estimate a~value function
          \[
            v^p(s) = \E [z_t | s_t = s, a_{t \dots T} \sim p]
          \]
          that predicts the outcome from position~$s$ (of~games played by~using policy $p_\rho$)
        \item Specifically, $v_\theta(s) \approx v^{p_\rho}(s) \approx v^*(s)$.
        \item task of~\textbf{regression}
        \item stochastic gradient descent:
          \[
            \Delta \theta \propto \frac{\partial v_\theta (s)}{\partial \theta} (z - v_\theta(s))
          \]
          {\tiny (to~minimize the mean squared error (MSE) between the predicted $v_\theta(s)$ and the true $z$)}
      \end{itemize}
    \end{frame}

    \begin{frame}{Value Network (2/2)}
      Beware of~overfitting!
      \pause
      \begin{itemize}[<+- | alert@+>]
        \item Successive positions are strongly correlated.
        \item Value network memorized the game outcomes, rather than generalizing to new positions.
        \item Solution: generate 30 million (new) positions, each sampled from a~\textbf{seperate} game
        \item almost the accuracy of~Monte Carlo rollouts (using $p_\rho$), but $15000$ times less computation!
      \end{itemize}
    \end{frame}

    \begin{frame}{Selection of~Moves by the Value Network}
      \begin{center}
        \includegraphics[height=.8\textheight]{../img/move_selection_by_value_network.png}

        \tiny
        evaluation of~all successors $s'$ of~the root position~$s$, using~$v_\theta(s′)$
      \end{center}
    \end{frame}

    \begin{frame}{Evaluation accuracy in~various stages of a~game}
      \includegraphics[width=\textwidth]{../img/policies_move_numbers_vs_MSE.png}

      \vskip -2.4ex
      {\tiny
      \textbf{Move number} is the number of~moves that had been played in the given position.
      }

      \pause
      Each position evaluated by:
      \begin{itemize}[<+- | alert@+>]
        \item forward pass of the value network~$v_\theta$
        \item 100 rollouts, played out using the corresponding policy
      \end{itemize}
    \end{frame}

    \begin{frame}{Training the (Deep Convolutional) Neural Networks}
      \begin{center}
        \includegraphics[width=\textwidth]{../img/neural_nets_pipeline.png}
      \end{center}
    \end{frame}

    \begin{frame}{ELO Ratings for~Various Combinations of~Networks}
      \begin{center}
        \includegraphics[height=.85\textheight]{../img/ELO_ratings_various_combinations_of_ANNs.png}
      \end{center}
    \end{frame}
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{Poker: Cepheus, DeepStack}
  {
    \setbeamertemplate{frame footer}{\cepheusCitation}
    \begin{frame}{Cepheus\\
        \tiny Heads-up Limit Hold’em Poker}
      \pause
      \begin{center}
        \includegraphics[width=\textwidth, keepaspectratio]{../img/limit_holdem_poker.jpg}

        \url{http://poker.srv.ualberta.ca/}
      \end{center}
    \end{frame}
  }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{Beyond DeepStack: TensorCFR}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \section{Conclusion}

  \begin{frame}[standout]
    \begin{center}
      Thank you!

      Questions?
    \end{center}
  \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \appendix
  \begin{frame}[standout]
    Backup Slides
  \end{frame}

  {
    \setbeamertemplate{frame footer}{\alphaGoCitation}
    \begin{frame}{SL Policy Networks (1/2)}
      \begin{itemize}[<+- | alert@+>]
        \item 13-layer deep convolutional neural network
        \item goal: to~predict expert human moves
        \item task of \textbf{classification}
        \item trained from 30 millions positions from the KGS Go Server
        \item stochastic gradient ascent:
          \[
            \Delta \sigma \propto \frac{\partial \log p_\sigma (a|s)}{\partial \sigma}
          \]
          {\tiny (to~maximize the likelihood of~the human move~$a$ selected in~state~$s$)}
      \end{itemize}
      \pause

      Results:
      \pause
      \begin{itemize}[<+- | alert@+>]
        \item $44.4\%$ accuracy (the state-of-the-art from other groups)
        \item $55.7\%$ accuracy (raw board position + move history as~input)
        \item $57.0\%$ accuracy (all input features)
      \end{itemize}
    \end{frame}

    \begin{frame}{SL Policy Networks (2/2)}
      Small improvements in~accuracy led to~large improvements in~playing strength (see the next slide)
      \begin{center}
        \includegraphics[width=\textwidth]{../img/SL_policy_accuracy_vs_win_rate.png}
      \end{center}
    \end{frame}

    \begin{frame}{RL Policy Networks (details)}
      Results (by sampling each move $a_t \sim p_\rho(\cdot | s_t)$):
      \pause
      \begin{itemize}[<+- | alert@+>]
        \item $80\%$ of~win rate against the SL policy network
        \item $85\%$ of~win rate against the strongest open-source Go program, \textbf{Pachi} (\cite{Baudivs2011pachi})
          \begin{itemize}[<+- | alert@+>]
            \item The previous state-of-the-art, based only on~SL of~CNN: \\
              \pause
              $11\%$ of~``win'' rate against Pachi
          \end{itemize}
      \end{itemize}
    \end{frame}
  }

  % TODO display (I), (II) instead of i, ii
  \begin{frame}[allowframebreaks]{Further Reading}
    \tiny
    % TODO fill in relevant Further Reading
    % Machine Learning:
    % \begin{itemize}
    %   \item \textbf{Deep Learning} (\cite{Lecun2015deep})
    %   \item \textbf{Deep Learning course} \url{https://www.udacity.com/course/deep-learning--ud730}
    % \end{itemize}
  \end{frame}

  \begin{frame}[allowframebreaks]{References}
    \tiny
    \printbibliography[heading=none]
  \end{frame}

\end{document}
